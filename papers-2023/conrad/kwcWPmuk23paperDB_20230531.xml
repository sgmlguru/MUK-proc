<?xml version="1.0" encoding="UTF-8"?><article xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink" version="5.0" xml:lang="en"><info><title>Word processing is so last century</title><subtitle>Formalizing internal narratives using internal declarations and making them look pretty</subtitle><author><personname>Kurt Conrad</personname><email>conrad@SagebrushGroup.com</email><uri>http://@SagebrushGroup.com</uri><personblurb><para>Kurt starts stuff up. Conversations. Improvisational ensembles. Communities of practice. Nuclear reactors and SGML training programs for the US Department of Energy. XML systems and new organizations for many others.</para><para>He's found that getting everyone on the same page before designing and engineering page-production pipelines speeds progress. His latest work involves using bottom-up, multi-perspective decision making techniques to voice both community values and semantic values in order to develop agreements and bots that better-manage complexity and accelerate change.</para><para>In this paper, Kurt attempts to describe his personal exploration into using markup to make sense of things by voicing the perspectives of an often-confusing semantic technology researcher and a second persona, the flippant Test subject.</para></personblurb><affiliation><jobtitle>Principle researcher &amp; Test subject</jobtitle><orgname>The Sagebrush Group</orgname></affiliation></author><keywordset><keyword>Authoring markup</keyword><keyword>Knowledge management</keyword><keyword>Sense making</keyword><keyword>Semantic formalization</keyword><keyword>Ontologies</keyword><keyword>Bottom-up negotiations</keyword><keyword>Collaborative communications</keyword></keywordset><abstract><para>This presentation describes a journey of replacing WordPerfect with prodoc.dtd, a semantic authoring doctype; and how prodoc evolved to enable computer-assisted sense making, based on markup that formalizes knowledge flow analysis and modeling semantics.</para><para>The paper also describes some of the challenges associated with multi-perspective decision making and techniques for negotiating and formalizing dynamic, natural-system ontologies. Lessons learned from WordNet and SUMO integrations are summarized.</para><para>The paper concludes by suggesting that building editors and bots that use author-authored markup to digitize and amplify logic systems could, in many organizational settings, contribute to more-intelligent value optimizations and better long-term performance.</para></abstract></info><section><title>Executive Summary</title><para>Sense making is all about knowing what to do in a given situation, both individually and collectively. Many observe problems with our capacity for collaborative, intelligent behavior. As automated systems (bots) get more involved in decision making, the world must be described more explicitly for intelligent, automated behavior.</para><para>This is where natural-system and formalized ontologies come into play. </para><para>This paper focuses on using markup to capture the natural languages that individuals use when they make sense of the world — the languages of internal narratives — to start negotiations around formalizing natural system ontologies into automated support systems to help individuals and groups make sense of complexity from multiple perspectives.</para><para>Formalizing an individual's personal ontology involves digitizing the way that they think about and organize information. This paper reports lessons learned from experiments developing a personalized XML doctype, prodoc. After being modified, as needed, for over a decade, prodoc has evolved to ease the authoring of information from many different perspectives/ logical systems/ ontologies.</para><blockquote><attribution>Test subject</attribution><para>What makes for a better bot? One that's fast, cheap, low-risk, easily trained, and thinks like me.</para></blockquote><para>The first section, <citetitle>What's computer-assisted sense making?</citetitle> looks at the role of automation in helping see patterns and organizing actionable information to enable the intelligent behavior of individuals, groups, and bots. It starts by relating semantic markup, semantic technologies, and formalized ontologies to human and automated sense making processes.</para><para><citetitle>The path to prodoc</citetitle> contrasts word processing's focus on visual formatting behaviors and storage models with generalized markup, where text files contain start and end tags that describe nested containers that make content easy to process and stylesheets provide visual formatting specifications.</para><para>It introduces semantic markup, which involves giving the containers meaningful names, and semantic authoring, which allows authors to easily create new markup and name the data structures, themselves. Technically simple, allowing authors to declare new markup within documents has significant policy and value-proposition impacts.</para><para><citetitle>prodoc in practice</citetitle> describes computer-assisted sense making tools that a test subject developed using custom markup and element/ attribute-based control surfaces. These control surfaces, sometimes augmented with form controls, allow the author to easily change and adjust CSS visual rendering properties. Sheet music, engineering accessible color pallets, and a modeling language are described.</para><para><citetitle>The politics of markup and meaning</citetitle> deals with politics, which is defined as the way that groups make decisions. Complex global publishing lifecycles bring countless perspectives to the negotiation table. When semantic formalization and authoring are viewed from this perspective, semantics get dynamic. People change their minds about what's meaningful and how to communicate it.</para><para>A generalized process for values-based decision making is introduced, and its use in stabilizing and formalizing ontologies during markup development is described. Possible roles for semantic authoring in bottom-up data negotiations are considered. The results from mapping personalized markup to authoritative third-parties, WordNet and SUMO, are reported.</para><para>The paper concludes by introducing H1, a semantic authoring training system that is available for testing.</para></section><section><title>What's computer-assisted sense making?</title><para>Sense making is all about knowing what to do in a given situation, both individually and collectively. Many observe problems with our capacity for collaborative, intelligent behavior. </para><para>Computers can assist with communications. Likewise, they can assist with sense making, helping people organize information, visualize information, see patterns, and make decisions. As automated systems (bots) get more involved in decision making, the world must be described more explicitly for intelligent, automated behavior.</para><section><title>Onto-what?</title><para>What does it take to explain everything to a computer? This is where the ways that we perceive existence (natural-system ontologies) and formalized computing ontologies come into play. What's an ontology? Here are a couple of descriptions:</para><blockquote><attribution>W. V. O. Quine</attribution><para><phrase role="artifact">Ontology is the subject that asks the question <quote>What is there?</quote></phrase></para><para><phrase role="artifact">The answer can be stated in one word: <quote>Everything.</quote></phrase></para></blockquote><figure><title>Sowa Hexagon</title><mediaobject><alt>The Sowa Hexagon relates language to the world and to ontologies</alt><imageobject><imagedata align="center" fileref="img/codeblock.sowa.png" scale='50'/></imageobject></mediaobject></figure><para>John F. Sowa's Hexagon puts language at the center and emphasizes the importance of mental models and logic in the formalization of an ontology. Formalizing real-world and abstract concepts for our buddies the bots effectively involves not only dealing with <quote>what exists?</quote> as a list of terms, but also by dealing with all of the models and behaviors that Sowa identifies.</para><para>Fully-formalized computing ontologies associate terms with formal logic. That's really expensive. Markup, from this perspective, could be considered a semi-formalized ontology.</para><figure><title>Formalized ontologies</title><mediaobject><alt>Formalized ontologies associate behavioral logic directly with terminology</alt><imageobject><imagedata align="center" fileref="img/kfam.formal.png" width="100%"/></imageobject></mediaobject></figure><figure><title>Semi-formalized, markup-based ontologies</title><mediaobject><alt>Markup formalizes the terminology but not behavioral logic</alt><imageobject><imagedata align="center" fileref="img/kfam.semiformal.png" width="100%"/></imageobject></mediaobject></figure></section><section><title>Working with multiple languages, perspectives, and ontologies</title><figure><title>Conrad corollary</title><mediaobject><alt>Languages reflect different ontology-perspectives of the world</alt><imageobject><imagedata align="center" fileref="img/codeblock.conrad.png" width="30%"/></imageobject></mediaobject></figure><para>The Conrad corollary to Sowa's hexagon sees the world awash in languages that reflect different ontological perspectives, concepts, and logic systems. These are reflected in terminology and usage. Some are naturally-occurring. Some are engineered. Some are dysfunctional. Modern communication systems allow them to compete, globally, for attention and influence.</para><para>attempted to integrate and synthesize multiple ontologies and associated languages:</para><itemizedlist><listitem><para>Word processing and its visual concept of the document, which focuses on how text is arranged and looks on a page, and the ad-hoc, often-redundant behaviors associated with applying layout and style formatting rules with a mouse.</para></listitem><listitem><para>The UNIX tradition of little languages and little bots for text processing.</para></listitem><listitem><para>Semantic markup, SGML, XML, etc., which are based on IBM's hierarchical concept of the document. </para></listitem><listitem><para>Knowledge management and engineering. Concepts for describing the fractal ways that data is transformed into information and synthesized into actionable knowledge to enable intelligent, intentional behavior. Data's about the details. Information's about organizing. Knowledge's about actionable behaviors.</para></listitem><listitem><para>Multiple operational ontologies for concepts of semantics and meaning. What <quote>meaning</quote> means for humans, automated, and organizational systems. How semantic technologies and formalized ontologies can be used to engineer knowledge about and understanding of meaning in these different behavioral contexts..</para></listitem><listitem><para>Policy making and performance. Techniques for helping folks articulate perceived values to rapidly reach agreements around terms, meanings, priorities, and operational details. The vocabularies of the Government Performance and Results Act, activity-based costing, and value canvasses.</para></listitem></itemizedlist><blockquote><attribution>Test Subject</attribution><para>Remember, style is something that things fall out of, and relational database tables are OK for data, but didn't <emphasis role="bold">and don't</emphasis> work for organizing documents.</para></blockquote></section><section><title>How prodoc helped make sense of big words</title><para>The test subject's experiments in computer-assisted sense making involved using markup to</para><itemizedlist><listitem><para>Combine concepts and techniques to model associations</para></listitem><listitem><para>Analyze and refine those models to design and debug knowledge flows and computing systems</para></listitem><listitem><para>Develop ad-hoc and switchable visualizations to identify, highlight, and communicate informative patterns</para></listitem><listitem><para>Iteratively-refine markup and support systems based on new priorities and learnings</para></listitem><listitem><para>Negotiate <quote>optimal</quote> balances of operational and management considerations, such as priorities and quality criteria, author effort, development effort, markup flexibility and specificity, cool examples, schedule, lifecycle costs, and performance</para></listitem><listitem><para>Build agile, continuous delivery pipelines that enable anything, anytime, modifications to structure, layout, look, and feel; producing highly-contextualized PDFs and invalid, funny-looking source documents.</para></listitem></itemizedlist></section><section><title>Lessons learned</title><para>Co-locating knowledge and behavior tends to improve performance. Multiple perspectives tend to improve quality (and avoid crashes). The big challenge is how to integrate multiple perspectives and synthesize holistic, intelligent behavior.</para><para>Markup brings important capabilities to the table. It helps with the mechanics of getting knowledge to the agents responsible for behavior. It's both human and machine readable, and makes for an excellent negotiation framework.</para><para>Knowledge gaps to be identified and resolved through analysis of knowledge flows and language patterns. Plugging knowledge gaps better align behaviors and improve overall performance. Knowledge flows can be engineered to enable specific organizational behaviors and disable others, either through knowledge gaps or cost drivers. That's quite the ticket when associated with behavior prediction markets.</para><para>Shared meaning is negotiated. Meaning has multiple dimensions. When multiple agents interact with knowledge, they bring multiple perspectives. Just for starters, there's the plumbing side, how to move knowledge between agents.</para><para>Adding the policy and performance perspectives brings, <quote>Why inform them?</quote><quote>Who benefits?</quote><quote>What are the costs?</quote> The list of unique stakeholder perspectives can be countless, especially after folks apply new logic systems after changing their minds.</para><para>Knowledge architectures relate the bits and pieces of how knowledge enables behavior. Markup can be used in a knowledge architecture not only to enforce top down data quality standards, but enable the bottom-up negotiation of complex systems.</para><para>One way to use markup to integrate multiple-perspectives is by capturing the natural languages that individuals use when making sense of the world — the languages of internal narratives — and using those languages to start negotiations around formalizing natural system ontologies into automated support systems (to improve ergonomics and operational performance) and establishing community standards (for management system performance).</para><para>Formalizing an individual's personal ontology involves digitizing the way that they think about and organize information. This paper reports lessons learned from experiments developing a personalized XML doctype, prodoc. After being modified, as needed, for over a decade, prodoc has evolved to ease the authoring of information from many different perspectives/ logical systems/ ontologies.</para><para>The development of specialized data structures is at the essence of the idea of computer-assisted sense making. Organizing information, both structurally and visually, enables patterns to be identified and logic systems applied to enable behavior. When the situation makes sense, you know what to do.</para><para>Bots can do many things to help things make sense. Many experiments involved fine tuning the visual characteristics of authoring interfaces to make authoring as ergonomically-comfortable and time-efficient as possible.</para><para>At times, multi-perspective visualizations conflict with each other, potentially damaging signal-to-noise ratios. Switches are useful to help visualize patterns, even shifting layouts and visual style mappings to highlight different aspects, as appropriate. Explicitly-structured markup makes these types of processing and rendering tricks much easier to operationalize.</para></section></section><section><title>The path to prodoc</title><para>prodoc is the synthesis of:</para><itemizedlist><listitem><para>Some relatively simple things (e.g., word processing)</para></listitem><listitem><para>Some surprisingly simple things (e.g., helping people figure out what the agree on)</para></listitem><listitem><para>Some medium complexity things (e.g., markup systems)</para></listitem><listitem><para>And some somewhat more-complex things (e.g., understanding knowledge flows from multiple perspectives)</para></listitem></itemizedlist><blockquote><attribution>Test subject</attribution><para><quote>No, prodoc's not available. It's an ecosystem of polished production systems and abandoned experiments. More importantly, you'd have to think like me. That process would likely hurt both of us.</quote></para></blockquote><section><title>Bots hate word processing's ornery visually-oriented ontology</title><para>Let's start with the familiar. Imagine building a spreadsheet with only three columns: <emphasis role="bold">bold</emphasis>, <emphasis>italic</emphasis>, and <emphasis role="underline">underline</emphasis>? How would you make sense of anything?</para><para>Word processing (WP) is an approach to documents that is firmly planted in the world of paper. It started by managing whitespace and adding a few visual formatting tricks to replace typewriters. Apple dramatically focused attention on defining the document in terms of its visual characteristics when it introduced the Macintosh, with synchronized bitmapped displays and printers.</para><para>We're still largely at that stage, where the bulk of the world's documentation is encoded in proprietary formats that concern themselves mostly with visual formatting of information on physical and virtual pages. Even when the files are encoded using marked-up text, the metadata and other metaknowledge artifacts store mostly visual properties.</para><para>WP's visual orientation is problematic. It really requires humans to be in the loop to look, see, recognize, interpret, synthesize with other understandings to apply missing implicit knowledge, decide, and act.</para><figure><title>Word processing</title><mediaobject><alt>Word processing is optimized to drive screens and printers to feed eyes</alt><imageobject><imagedata align="center" fileref="img/kfam.visualWP.png" width="100%"/></imageobject></mediaobject></figure><para>The visual metaphors also limit the ability of computers to both mediate human-to-human communications and also to participate in the conversation. Even the XML-based word processing encodings are rats nests of difficult-to-process markup. The noise of visual parameters swamps the signal of individual characters. Doable, but expensive.</para><para>And this is the bottom line of why WP is obsolete technology. It's just too expensive. Labor costs are too high, in large part because you can't build happy little bots to automate anything. You have to buy bots or hire someone to build big grumpy bots.</para><para>WP provides flexibility to easily tune appearance, but reuse? Global imperatives require fine-grained, context-sensitive documentation to drive intelligent behavior. That means automation. That means containers. Sure, you can move files, but you can't move fine-grained information in and out of narrative documents. WP data is, to quote someone I met at a non-destructive nuclear assay conference, <quote>just too stupid to move.</quote></para><para>And this isn't the way that the big dogs roll. The publishing industry started walking away from this technology, decades ago.</para></section><section><title>Semantic markup to the rescue</title><para>In the 1990s professional publishers were getting cut up worse than the weak kid at a knife fight. Today, news publishers are really bleeding out.</para><para>The world of electronic documents was disrupting paper-based publishing. New terms, new technologies, and new processes forced new business logic and strategies. CD-ROMs hit first. Many organizations made significant investments to reengineer their documents to rapidly-release CD-ROMs.</para><para>Then the web hit, with its wicked learning. Publishers asked themselves, <quote>How many times would this happen?</quote><quote>How expensive would conversion costs be for each new technology cycle?</quote><quote>How many different file formats will we need?</quote><quote>How many different variations for different delivery devices?</quote></para><para>Fortunately, the web came with the seeds of the solution. The format used for web documents, the HyperText Markup Language (HTML), was based on an international standard, the Standard Generalized Markup Language (SGML). SGML defines rules for declaring, applying, and validating markup languages — a language for defining markup languages — a <quote>meta-language</quote> so to speak.</para><para>SGML was designed around the needs of document owners, authors, and publishers. It enables hub and spoke architectures to be implemented with authoritative, single sources of generalized markup that feed various delivery systems (CD-ROMs, web, etc.).</para><section><title>Separating structure and style</title><para>Fundamental to generalized markup is the separation of content from style. It leverages lessons learned that go back to John W. Seybold's work at ROCAPPI, where lookup tables were first used to replace complex and error-prone typesetting markup codes with short — easy to type and differentiate — strings of text.</para><figure><title>Typesetting lookup table</title><informaltable><thead><tr><th>Easy strings of text</th><th>Confusing, error-prone typesetting codes</th></tr></thead><tbody><tr><td>"<computeroutput role="xst">&lt;p&gt;</computeroutput>"</td><td>;;; ldasj;fsadlkf dsalkf jdsa;l fjdsl;f dsl;f jdsafldslfkds ld :::</td></tr><tr><td>"<computeroutput role="xet">&lt;/p&gt;</computeroutput>"</td><td>;;; a s;dfkjsadfklds fldskaj fdskf ds;lkf sdlkf dslf sadlfk jdsfkj ds :::</td></tr></tbody></informaltable></figure><para>In this world, formatting is associated with generalized data structures, making it easy to apply new styles or transforms, as the need arises. For this document, it took a couple of days to:</para><itemizedlist><listitem><para>Add new data structures to prodoc to make authoring easier to match DocBook requirements.</para></listitem><listitem><para>Create a new stylesheet, <filename>pdoc2docbook.xsl</filename>, to transform prodoc XML to DocBook XML.</para><para>It stripped all of prodoc's attributes, mapped element names from one markup language to the other, and changed the structure of a few branches.</para></listitem><listitem><para>Tune the XML source and XSLT files to create valid DocBook XML.</para><para>prodoc structures that couldn't be rendered were converted to images.</para></listitem></itemizedlist><para>Once the pipeline was stable, attention went back to authoring and editing. A handful of whitespace issues required manual cleanup.</para></section><section><title>Integrating lifecycle perspectives</title><para>The focus on document lifecycles drove a number of important features that are missing from contemporary word processing platforms and relational database systems.</para><para>Markup language files are text files. That simple fact improves accessibility and lowers lifecycle costs by orders of magnitude. Nearly any computing device and system can interact with a markup-encoded text file. The Extensible Markup Language (XML) dramatically lowered SGML's cost profile, and there are a wealth of XML-based markup technologies.</para><para>Documents are naturally hierarchical. Books contain chapters. Chapters contain paragraphs... Markup is organized around addressable trees and nodes and nested containers. These explicit data structures lower level-of-effort for both human and machine processing.</para><para>Semantics enter the picture when naming the nodes in the tree. When creating custom data structures to solve real-world authoring and publishing issues, the names are often associated with real-world meanings and organizing concepts. These terms, their meanings, and various encoding decisions take us into the world of semantic technologies and formalized ontologies.</para><para>Semantic markup is the phrase given to this approach to document management, where XML is used to define an application-specific markup language, where the names and data structures reflect complex conceptual frameworks and models.</para><para>The challenge when using markup for single sources is to focus all authoring on the single, highly-refined source document, so that all derivatives flow from there.</para><blockquote><attribution>Test subject</attribution><para>Hub and spoke, baby, with low cost transforms. Hub and spoke.</para></blockquote></section></section><section><title>From semantic markup to semantic authoring</title><blockquote><attribution>Test subject</attribution><para>prodoc helps me make sense of the world. Markup brings ergonomic efficiencies to the typing process. Semantic markup brings ergonomic efficiencies to the thinking process, especially when concepts and logic systems can be visualized in different ways.</para></blockquote><para>Most semantic markup systems are organized around the idea of collaborative authoring and publishing. An organization-specific markup language is defined as a doctype with a set of rules that describe the names and relationships of the various containers. Everyone in the group works within those rules, which function as quality standards.</para><para>Doctypes are designed to standardize documents and protect publishing systems. The structural rules, which are described using an explicit schema language, typically change very slowly to ensure consistency of work processes. Standardized stylesheets are usually part of the equation.</para><para>Semantic authoring inverts this thinking.</para><para>Authors are the semantic authorities, they should have all the operational authorizations available to author semantic markup as they deem appropriate, as easily as possible. Every doctype should allow authors to use the internal declaration subset to enable bottom-up formalization and communication of models, in addition to content.</para><para><emphasis role="bold">Semantic authoring is about creating documents that help authors make sense of the world. It lets individuals create semantic markup to formalize their conceptual frameworks. Once formalized, these sense making systems are easier to reuse, communicate, and automate.</emphasis></para><para>Reducing the incremental costs of new markup means document-level extensions, making every document a <phrase role="gloss"><emphasis role="bold">little markup language laboratory</emphasis><emphasis role="treatment"> The idea of <quote>little languages</quote> comes from AT&amp;T's development of the UNIX operating system, where little languages and little bots were developed for managing the streams of text associated with managing telecommunication systems in the 1970s</emphasis></phrase>.</para><para>Semantic authoring doctypes leverage the Internal Declaration Subset, which is a mechanism for authors to create their own markup declarations. This allows new elements and attributes to be declared within any document. They can even be used within the current document, if the rules of the doctype allow.</para><para>prodoc was built on an HTML core (HTML for semantic authoring: hsa.dtd), but comparable semantic authoring extensions could be incorporated into other doctypes. Adding semantic authoring extensions to a DITA subset have been discussed, for example.</para><para>Why open Pandora's Box? What chaos would ensue?</para><section><title>Individual impacts</title><para>Any number of strategic computing projects have failed because organizations couldn't get folks to use XML editors.</para><para>Why would an individual consider this craziness? Everyone's plate is full. This requires learning and ever since Microsoft took the margins out of WP through bounding, investments in this space have withered, for both individuals and organizations. WP is an ignored necessity, driven by cost-minimization strategies.</para><para>The standard XML consultant's answer to any question is, <quote>That depends.</quote> Markup is largely policy neutral. You can apply it to almost any set of value optimizations. Markup languages are adopted, customized, and designed around a purpose. When you use someone else's markup, you are organizing your information around someone else's semantic values and value propositions. That's a cost driver.</para><para>Everyone's plate is full because the world is increasingly complex. There are a lot of bots that help, but they're all pretty much built by someone else to help them meet their needs.</para><para>What if folks had bots of their own? Little bots that could be put together in an afternoon? Organized around personally-valuable information. Individuals creating their own little languages and bots inverts markup's dominant value propositions.</para><figure><title>Generalized markup</title><mediaobject><alt>Markup is processed into trees. Those trees can be rendered for display using CSS. They can be converted to other languages to interact with the world.</alt><imageobject><imagedata align="center" fileref="img/kfam.markup.png" width="100%"/></imageobject></mediaobject></figure><para>When concluding XML courses I challenge students to build their own <quote>bootcamp app</quote>:</para><itemizedlist><listitem><para>An XML fragment of information that has personal meaning.</para></listitem><listitem><para>Some CSS to make it pretty and a DTD to validate it.</para></listitem><listitem><para>Extra credit for an XSLT to HTML conversion. With that set of languages you can code the guts of almost any automated solution.</para></listitem><listitem><para>Pro tip: Add a text processing language to get stuff into markup.</para></listitem></itemizedlist><blockquote><attribution>Test subject</attribution><para><quote> AWK is awful good.</quote></para></blockquote><para>Reports ranged from ham radio datasets to a DVD collection with a color scheme that would drive a person to drink. The common denominator? Feelings of accomplishment and excitement about the future. Personal markup is compelling. It's the ultimate computer game.</para><para>When markup is optimized around individual purpose, it's an enabler, with corresponding changes to brain chemistry. A recent episode of the Public Broadcasting Series, Nova, focused on the neurochemistry of decision making.</para><blockquote><attribution>Test subject</attribution><para>Agency is powerful stuff. Life is unscripted. Perceptions of control are powerful. Actual, impactful, personally-meaningful operational control? Priceless.</para></blockquote><para>The brain chemistry associated with fear is another important factor. Fearful adults staring at blank screens with their jobs on the line don't learn much. Fearful improvisers tend to get frantic. Open another document template and experiment with markup is like playdough and enables much more creativity.</para><blockquote><attribution><link xl:href="https://slate.com/podcasts/amicus/2021/02/incitement-impeachment-inevitable-acquittal.amp">Anat Shenker-Osorio</link></attribution><para> ...when people are made afraid, their amygdala starts firing and their prefrontal cortex literally is starved of blood. You can’t have both things [fear and logic] going. And so if you’re asking people to sort of be in their rational brain..., then you need to present this as the possibility of creating something good... </para></blockquote><blockquote><attribution>Primary researcher</attribution><para>Future research hopes to work with neuro-divergent individuals, including those with sensory-integration impairments who have had to create very sophisticated coping mechanisms and supporting ontologies. How much of this knowledge is tacit and inexpressible? How much could be formalized as language and would there be benefits? Stories heal. Would this form of storytelling amplify and add additional dimensions to those benefits?</para></blockquote><para>Giving voice involves two dimensions, the content and the pipeline that communicates that voice. Authoring content that is designed for collaboration and integrates easily with existing computing infrastructures adds social dimensions to individual value propositions.</para><blockquote><attribution>Test subject</attribution><para>Our voices are so valuable that we're the only animal that chokes itself to death. Would average individuals actually use markup to model their realities? Look at what they do with spreadsheets and the various cloud integration toolkits. This is an accessibility/ price/ performance thing.</para></blockquote></section><section><title>Organizational impacts</title><blockquote><attribution>Doris Lessing, author </attribution><para>Humanity’s legacy of stories and storytelling is the most precious we have. All wisdom is in our stories and songs. A story is how we construct our experiences. At the very simplest, it can be: ‘He/she was born, lived, died.’ Probably that is the template of our stories – a beginning, middle, and end. This structure is in our minds.</para></blockquote><para>Markup has strategic organizational impacts. That's why global publishers use it.</para><para>Markup helps authors make sense of the past, present, and future. It makes it easier to organize information for efficient, increasingly-customized and personalized pipelines. This is human behavior we're trying to influence. We're not just dealing with data.</para><figure><title>Authoring meaning through content, structure and style</title><programlisting><emphasis role="bold">Past meaning      \   Observe    / Future meaning
Established values \     O      /  Future values
Past practice       \    ||    /   Future behavior
Source origins      /    /\    \   Downstream k flows
Old logic          /   Orient   \  Re-contextualized logic
Event &amp; prior K   / Decide &amp; Act \ Learnings</emphasis></programlisting></figure><para>Markup is good for top-down communications and alignment. If top-down alignment systems were sufficient, our existing single-source-based communication and publishing systems should be ensuring top-notch performance.</para><para>But operational realities in countless natural economies point to the need for fundamental changes in the rules of the symbolic/ market economy. That appears to be becoming increasingly difficult as the mechanisms that influence market behavior appear to be cutting themselves off from any communications that might challenge the current systems for monetizing human behavior.</para><para>Through countless decisions, systems evolve and are optimized around specific value propositions. This has happened to markup systems and associated technologies. The ISO:SGML platform, quite simply, establishes a benchmark standard for open systems. XML's refinements have brought countless new voices to the table, but author-authored markup has simply not gotten the same attention as single-sourcing and those value optimizations are reflected in the available technology alternatives through many small details that increase authoring effort in subtle, but impactful ways.</para><para>Bottom-up sense making and communications are becoming more important. Enabling individuals to customize markup in an organizational setting can be expected to not only increase reuse and collaboration, but also better-enable multi-perspective systems and decisions.</para><para>Formalizing individual value-optimizations as semantic values shares many traits with fundamentally-enabling technologies, like email. It opens up new channels of communication. How many organizations justified their initial investments in email with its impact reducing paper mail. That happened, but it didn't anticipate the completely-new models of communication that were enabled.</para><blockquote><attribution>Primary researcher</attribution><para>The big experiment: Can bottom-up knowledge flows be established quickly-enough to build the new understandings and consensus around the logic needed to rapidly realign global supply chains around strategic physical and operational constraints.</para></blockquote></section></section></section><section><title>Document-level controls to capture and communicate meaning</title><blockquote><attribution>Test subject</attribution><para>Semantic authoring is like car restoration on <citetitle>Full Custom Garage.</citetitle> The markup is the frame, and the CSS is the sheet metal. Pound at will. Make the data dance and drive change.</para></blockquote><para>prodoc provides two primary ways to influence the meaning of information: data structures and visualizations.</para><para>The basic process for making sense of new data?</para><orderedlist><listitem><para>Do existing structures work? If so, adapt, otherwise...</para></listitem><listitem><para>Add markup to give the data structure</para></listitem><listitem><para>Pick meaningful names</para></listitem><listitem><para>Add visual differentiation</para></listitem><listitem><para>Adjust the look and geometry to look for patterns</para></listitem><listitem><para>Rinse and repeat until the data becomes actionable</para></listitem><listitem><para>Add a switch or something to change the appearance in real time, if it helps</para></listitem></orderedlist><para>Sometimes the process starts by making the data look pretty before creating custom data structures. The ability to incrementally add structures and style rules to the base platform reduces the incremental cost of small projects. Successful experiments get moved from the laboratory document to prodoc.dtd.</para><para>In practice, document-level experimentation has been key to the evolution of prodoc. The full case study document is the actual file that started the development of the <computeroutput role="xsol">&lt;music/&gt;</computeroutput> element.</para><para>Some of the design alternatives were driven by knowledge loss. The old markup didn't make sense. Create new markup that makes sense for current thinking. A few data structures only stabilized after multiple documents with partial solutions were created, sometimes across years. A couple of systems only went into production after the best features of 3 or 4 solutions from different perspectives were integrated.</para><section><title>Author-driven structural changes</title><blockquote><attribution>Kevin Kelly</attribution><para>Fast, cheap, and out of control</para></blockquote><para>Most publishing systems use slow moving schema and stylesheets to ensure consistency of authoring. Semantic authoring is intended to be fast moving. Allowing ideas to be quickly captured and different approaches tested. Creating ecosystems of markup alternatives, driven by personal and small-group imperatives.</para><blockquote><attribution>Test subject</attribution><para>Running off to a DTD file? Too far away, too slow, too impactful. I'm the authority here. Let me extend structures in the document.</para></blockquote><blockquote><attribution>Eric Cartman </attribution><para>Respect my authoritah</para></blockquote><para>This example of enabling SGML Internal Declaration Subset extensions comes from h1.dtd, a training doctype. An updated <orgname>oXml</orgname> framework will be released on June 10th, 2023.</para><para>Elements are organized into four non-overlapping groups: <computeroutput role="xpe">%divs;</computeroutput>, <computeroutput role="xpe">%blocks;</computeroutput>, <computeroutput role="xpe">%heads;</computeroutput>, and <computeroutput role="xpe">%spans;</computeroutput>, so that they can be easily extended and combined, at will. This is the example of the definitions for <computeroutput role="xpe">%divs;</computeroutput></para><figure><title><computeroutput role="xpe">%divs;</computeroutput> declarations includes the <computeroutput role="xpe">%sa.divs;</computeroutput> extension mechanism for use in documents</title><programlisting>&lt;!-- __________________________________________________________________________
. . {
. . {         %divs; - infinitely-nesting wrapper elements
. --&gt;
&lt;!-- ..........................................................................
. . :                 %h1.divs;         . baseline hierarchical divisions
. --&gt;
 &lt;!ENTITY % h1.divs           " div|
                                h1|
                                hsa|
                                hsg|
                                prodoc" &gt;
&lt;!-- ..........................................................................
. . :                 %sa.divs;         . semantic authoring interface
. --&gt;
 &lt;!ENTITY % sa.divs           "" &gt;
&lt;!-- ..........................................................................
. . :                 %divs;            . (roll-up)
. --&gt;
 &lt;!ENTITY % divs              " %h1.divs;
                                %sa.divs;" &gt;</programlisting></figure><figure><title>Document header, where new elements are added through the <computeroutput role="xpe">%sa.divs;</computeroutput> interface</title><programlisting>&lt;!DOCTYPE div PUBLIC "-//SG//DTD h1//EN" "../h1.dtd"[
&lt;!ENTITY    h1                "h1" &gt;
&lt;!ENTITY  % sa.divs           "" &gt;&lt;!-- e.g., "| ename" --&gt;
&lt;!ENTITY  % sa.blocks         "" &gt;
&lt;!ENTITY  % sa.heads          "" &gt;
&lt;!ENTITY  % sa.spans          "" &gt;
&lt;!ENTITY  % sa.atl            "" &gt;
&lt;!ELEMENT   ename             (#PCDATA) &gt;
&lt;!ATTLIST   ename
              aname           CDATA                                 #IMPLIED &gt;
]&gt;</programlisting></figure><para>A common reason for adding spans to a document is to customize tables. <computeroutput role="xsol">&lt;tr/&gt;</computeroutput> contains <computeroutput role="xpe">%spans;</computeroutput>, and the set of <computeroutput role="xpe">%spans;</computeroutput> includes <computeroutput role="xsol">&lt;td/&gt;</computeroutput> and <computeroutput role="xsol">&lt;th/&gt;</computeroutput>. Adding any element to the set of <computeroutput role="xpe">%spans;</computeroutput> adds them to table rows.</para></section><section><title>Author-driven visual changes</title><blockquote><attribution>Test subject </attribution><para><quote>Quickly changing the look and feel of text is WP's core value proposition. Pretty, but dumb. And it takes a lot of mousing around to set the properties on all those unrelated objects. Hurts my arm. Let me keep my keys on the keyboard, please. Let me apply style changes to branches and not leaves.</quote></para></blockquote><para>Since the semantic authoring platform was developed to displace WordPerfect, various markup-based style controls (control surfaces) were implemented.</para><para>Elements can function as visual control surfaces. <computeroutput role="xsol">&lt;p/&gt;</computeroutput> adds whitespace. <computeroutput role="xsol">&lt;t/&gt;</computeroutput> doesn't. <computeroutput role="xsol">&lt;frame/&gt;</computeroutput> adds a border. <computeroutput role="xsol">&lt;block/&gt;</computeroutput> doesn't. Mapping the <orgname>oXml</orgname><phrase role="oxcmd">Rename Element</phrase> function to Alt-N enables quickly changing how data looks by changing the element name. This frequently happens when tuning inline markup to adjust prominence.</para><para>Global attributes provide most of the visual control surfaces, overriding the CSS stylesheet defaults. Some of the simplest attributes are direct pass through. <computeroutput role="xa">@borders</computeroutput>, <computeroutput role="xa">@padding</computeroutput>, and <computeroutput role="xa">@style</computeroutput> accept standard CSS syntax. Many css properties were renamed to simplify authoring: <computeroutput role="xa">@bgcolor</computeroutput>, <computeroutput role="xa">@p.left</computeroutput>, <computeroutput role="xa">@p.right</computeroutput>, <computeroutput role="xa">@face</computeroutput> (typeface).</para><para>Some attributes add support for fixed values. <computeroutput role="xa">@scale</computeroutput>, for example, sets font-size using arbitrary values. <computeroutput role="xa">@sgscale</computeroutput> contains a standardized and generalized set of values based on the square root of the golden ratio. It started as a reference, for quickly bringing up a list of values, before being given an active role.</para><para><computeroutput role="xa">@color</computeroutput> and <computeroutput role="xa">@bgcolor</computeroutput> are defined with a number of named colors that map to a color library. One of the color pallets is for modeling knowledge flows. Another attribute, <computeroutput role="xa">@kstyle</computeroutput>, applies background, foreground, and border color combinations based on the value of the <computeroutput role="xa">@k</computeroutput> attribute, which describes the element's role in knowledge flows.</para><para>A few controls are more aspirational than functional. <computeroutput role="xa">@lineheight</computeroutput> never seems to work. <computeroutput role="xa">@max-height</computeroutput> and the pagination settings are waiting for more powerful engines.</para><para>A variety of <computeroutput role="xa">@sh*</computeroutput> attributes show and hide control panels, table grids, id values, purple numbers, included content, and <computeroutput role="xsol">&lt;xi:include/&gt;</computeroutput> controls. A couple more activate CPU-crushing focus and hover behaviors.</para></section></section><section><title>prodoc in practice</title><para>One day, the test subject was amused by the total absurdity of keeping project notes and analysis in WordPerfect (WP) while simultaneously writing XSLT specifications using an XML editor. The test subject rebuilt that specification doctype for a project and started modifying it to replace WP for all professional documentation. The customized authoring and publishing platform was named <quote>prodoc.</quote></para><para>Technically, prodoc is an <phrase role="gloss"><emphasis role="bold"/><emphasis role="treatment"/></phrase>, centered around <filename>prodoc.dtd</filename>, and a supporting toolkit, with many of the tools based on Oxygen XML Editor and Author software (<orgname>oXml</orgname>).</para><para>And so started a grand experiment. Could someone who had provided training, support, and engineering to thousands of WP users transition? </para><para>The only <quote>active</quote> WP document is business cards that are encoded in tables with offset crop marks.</para><blockquote><attribution>Test subject</attribution><para>Go back to WordPerfect? Are you crazy? Here, I can grab an element by pressing Alt-E. Even copying and pasting is easier. When I go to another editor, none of my compositional idioms are there. Yuck!</para></blockquote><blockquote><attribution>Test subject</attribution><para>I don't know how I could move these documents back to a word processor. Even accepting the loss of structure, the algorithmic visual effects would take too many mouse clicks to be worth the effort, and probably require a separate graphics package.</para></blockquote><!--<xinclude include="yes" shcontrol="show" shcontrols="show" shfilenav="show"
      shinclude="show" shp="show" shxi="show" shxicontrols="show">
      <xi:include href="D:\kc\comm\book\portfolio\music.pdoc" parse="xml"
        xml:base="D:\kc\comm\book\portfolio\music.pdoc"
        xmlns:xi="http://www.w3.org/2001/XInclude">
        <xi:fallback>
          <p color="red">Xinclude failure. File not found</p>
        </xi:fallback>
      </xi:include>
    </xinclude>--><section><title><computeroutput role="xa">@class</computeroutput> — Authored class styles</title><blockquote><attribution>Test Subject</attribution><para><quote>Look Ma. No stylesheet changes</quote></para></blockquote><para>An early design principle was, <quote>Hands off the <computeroutput role="xa">@class</computeroutput> attribute. That's for user control.</quote> Recently, an approach was implemented that allows authors to define classes and associate specific CSS styles, all within a document.</para><para>The <computeroutput role="xa">@style</computeroutput> attribute maps to the <orgname>oXml</orgname> -oxy-style CSS property. This allows raw CSS to be passed to the CSS processor for the defined element.</para><para>Adding a <computeroutput role="xa">@styleclass</computeroutput> attribute on an element makes the <computeroutput role="xa">@style</computeroutput> value available for reuse. A little XPath handles the indirection from <computeroutput role="xa">@class</computeroutput>, through the preceding <computeroutput role="xa">@styleclass</computeroutput>, to <computeroutput role="xa">@style:</computeroutput></para><figure><title><computeroutput role="xa">@class</computeroutput> markup</title><mediaobject><alt>The first &lt;t/&gt; element defines the style for the class. Others reference the defined styles through the @class attribute</alt><imageobject><imagedata align="center" fileref="img/codeblock.classMarkup.png" width="100%"/></imageobject></mediaobject></figure><figure><title><computeroutput role="xa">@class</computeroutput> CSS</title><mediaobject><alt>CSS associates the @style, @styleclass, and @class attributes</alt><imageobject><imagedata align="center" fileref="img/codeblock.classCSS.png" width="100%"/></imageobject></mediaobject></figure><figure><title><computeroutput role="xa">@class</computeroutput> rendering</title><mediaobject><alt>This figure demonstrates the CSS rendering through reference</alt><imageobject><imagedata align="center" fileref="img/tl.classRendering.png" width="50%"/></imageobject></mediaobject></figure></section><section><title><computeroutput role="xsol">&lt;awkbuddy/&gt;</computeroutput>
        — An interactive development environment block</title><para>One day, dreaming about the keyboard macro processors, I thought, <quote>AWK! of course, AWK!</quote> The result is <computeroutput role="xsol">&lt;awkbuddy/&gt;</computeroutput>, which inverts most code/ documentation conventions by putting code fragments into a standard prodoc.</para><figure><title><computeroutput role="xsol">&lt;awkbuddy/&gt;</computeroutput> codeblocks</title><mediaobject><alt>Shows the awk, source, target, and cmd codeblocks within an awkbuddy element </alt><imageobject><imagedata align="center" fileref="img/ull.awkbuddyStructure.png" width="40%"/></imageobject></mediaobject></figure><para>The code blocks can be arranged, as desired, to co-locate knowledge with decisions. I've tuned mine on eye movements, by wrapping the codeblocks in table structures. The <filename>awkbuddy.cmd</filename> script rips the prodoc into separate files and runs the pipeline. Factoring out the file-processing overhead made AWK a much more convenient tool. More little bots have been built. Much time saved. Much more complexity managed.</para><para>Perhaps more importantly, it improves knowledge retention and library accessibility. Simply having the code and a source fragment is usually sufficient to remember the project. On the other hand, if you want to write a novel to explain a few lines of AWK, the tools are at hand.</para><para>Custom IDs can be used to run multiple awkbuddies from the same prodoc and prevent collisions in the generated files. File-level collisions across separate awkbuddies (<filename>*.abud</filename>) never proved to be much of an issue. If the default target fragment has been overwritten, just rerun the transform.</para></section><section><title><computeroutput role="xsol">&lt;bbody/&gt;</computeroutput>, <computeroutput role="xsol">&lt;branches/&gt;</computeroutput>, <computeroutput role="xsol">&lt;branch/&gt;</computeroutput>
        — Hierarchical tables</title><para>The first hierarchical table was developed to do stakeholder analysis. The generalized implementation uses a specialized table body element: <computeroutput role="xsol">&lt;bbody/&gt;</computeroutput>. Changing <computeroutput role="xa">@display</computeroutput> switches between the full table and a simplified blocks view, which makes it easier to restructure the branches.</para><figure><title><computeroutput role="xpath">table/@display="table"</computeroutput> (columns displayed for data processing)</title><mediaobject><alt>Column one shows indentation to match the nested branches. Column two, the XPath. Column three, nothing important.</alt><imageobject><imagedata align="center" fileref="img/table.bbodyTable.png" width="100%"/></imageobject></mediaobject></figure><figure><title><computeroutput role="xpath">table/@display="block"</computeroutput> (columns hidden for tree processing)</title><mediaobject><alt>The same hierarchical table in block/ list mode</alt><imageobject><imagedata align="center" fileref="img/table.bbodyList.png" width="50%"/></imageobject></mediaobject></figure><figure><title><computeroutput role="xsol">&lt;bbody/&gt;</computeroutput> markup</title><programlisting>&lt;table shindent="show" hs="corner" display="block" shfocus="show"&gt;
  &lt;bbody&gt;
    &lt;branch&gt;
      &lt;tr&gt;
        &lt;th id="th"&gt;stakeholders&lt;/th&gt;
        &lt;td&gt;&lt;xpath&gt;table/bbody/branch/tr/td&lt;/xpath&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;branches&gt;
        &lt;branch&gt;
          &lt;tr&gt;
            &lt;th&gt;internal&lt;/th&gt;
            &lt;td&gt;&lt;xpath&gt;table/bbody/branch/branches/branch[1]/tr/td&lt;/xpath&gt;&lt;/td&gt;
          &lt;/tr&gt;
          &lt;branches&gt;
            &lt;branch&gt;
              &lt;tr&gt;
                &lt;th&gt;group&lt;/th&gt;
                &lt;td&gt;&lt;xpath&gt;table/bbody/branch/branches/branch[1]/&lt;/xpath&gt;&lt;xpath&gt;branches/branch[1]/tr/td&lt;/xpath&gt;&lt;/td&gt;
              &lt;/tr&gt;
              &lt;branches&gt;
                &lt;branch&gt;
                  &lt;tr&gt;
                    &lt;th&gt;individuals&lt;/th&gt;
                    &lt;td&gt;&lt;xpath&gt;table/bbody/branch/branches/branch[1]/&lt;xpath&gt;branches/branch[1]/&lt;xpath&gt;branches/branch[1]/tr/td&lt;/xpath&gt;&lt;/xpath&gt;&lt;/xpath&gt;&lt;/td&gt;
                  &lt;/tr&gt;
                &lt;/branch&gt;
              &lt;/branches&gt;
            &lt;/branch&gt;
          &lt;/branches&gt;
        &lt;/branch&gt;
        &lt;branch&gt;
          &lt;tr&gt;
            &lt;th&gt;external&lt;/th&gt;
            &lt;td&gt;&lt;xpath&gt;table/bbody/branch/branches/branch[2]/tr/td&lt;/xpath&gt;&lt;/td&gt;
          &lt;/tr&gt;
        &lt;/branch&gt;
      &lt;/branches&gt;
    &lt;/branch&gt;
  &lt;/bbody&gt;
&lt;/table&gt;</programlisting></figure></section><section><title><computeroutput role="xsol">&lt;colortest/&gt;</computeroutput>
        — Automating accessible color negotiation pipelines</title><para>Challenge: Use the W3C-published formula for calculating the accessibility of color combinations. Over many years, the system was re-factored. Copying and pasting values into a small XML model was fine for small projects, but I had to seek client approvals on a pallet of 24 colors that were going to be used for strategic branding.</para><figure><title>Color testing and tuning pipeline</title><mediaobject><alt>The pipeline enabled efficient tuning of color pallets</alt><imageobject><imagedata align="center" fileref="img/kfam.colorsPipeline.png" width="100%"/></imageobject></mediaobject></figure><para>The Just Color Picker tool generated HTML. A new transformation was created to map the HTML output to a new prodoc element, <computeroutput role="xsol">&lt;colortest/&gt;</computeroutput>. The production accessibility calculator was updated to handle the new schema and missing functionality was added.</para><para>Implementation had a mix of performance objectives: large swatches to make small comparisons, built in black, gray, and white samples, ability to instantly see color changes based on editable hex values, demonstrate single-sourcing concepts to client, save me an armload of effort</para></section><section><title><computeroutput role="xsol">&lt;h/&gt;</computeroutput>
        — Depth-based headings because big headings are ugly</title><para>HTML uses numbered headings, prodoc, on the other hand, uses nested divs with a single, context-sensitive <computeroutput role="xsol">&lt;h/&gt;</computeroutput> element</para><para>The following XPath scales headings based on the maximum depth of the document. <link xl:href="https://www.modularscale.com/?1&amp;em&amp;1.272">1.272 is the square root of the golden ratio</link>. A recent document needed a gentler scaling factor. The cube root of the golden ratio (<link xl:href="https://www.modularscale.com/?1&amp;em&amp;1.174">1.174</link>) was used, instead.</para><figure><title>CSS to scale headings based on document depth</title><programlisting language="css">h {
font-size : oxy_xpath("concat(
            math:pow(<emphasis role="bold">1.272</emphasis>,((max(//h/count(ancestor::*[@display='div'])) - count(ancestor::*[@display='div']) + 1 ))),
            'em')", evaluate,dynamic-once) ;
}</programlisting></figure></section><section><title><computeroutput role="xsol">&lt;kfam/&gt;</computeroutput>
        — An element and generalized design language to make sense of knowledge flows from multiple perspectives</title><blockquote><attribution>Test subject</attribution><para>The kfam ontology includes a lot of concepts, but you only really need to focus on three to make sense of knowledge flows: <inlinemediaobject><alt>agent in an orange box</alt><imageobject><imagedata fileref="img/agent.png" scale="40" valign="bottom"/></imageobject></inlinemediaobject>, <inlinemediaobject><alt>artifact in a blue box</alt><imageobject><imagedata fileref="img/artifact.png" scale="40" valign="bottom"/></imageobject></inlinemediaobject>, and <inlinemediaobject><alt>behavior in a green box</alt><imageobject><imagedata fileref="img/behavior.png" scale  ="40" valign="bottom"/></imageobject></inlinemediaobject>.</para></blockquote><para>The knowledge flow analysis and modeling language — kfam — represents the most extreme example of the fractal nature of this approach to computer-assisted sense making.</para><para>The base language, markup, and visualizations associated with kfam have evolved over 30 years of discussions around organizational performance. New concepts and ideas around those concepts interplay with new ways to organize and control the appearance of kfam data. The result is a design language, where kfam concepts are used to name elements, attributes, colors, etc.</para><para>One view of sense making is answering the question, <quote>How to make sense of this data? How to organize it without incessantly banging on the keyboard?</quote>. The meaning of kfam from this perspective centers on ergonomics, the behavior of eyes and hands. Operationally, that's tuning the markup, interfaces, and bots to organize data into actionable knowledge.</para><para>When multiple datasets need the same treatment, shifting from manual to automated processes makes sense. This type of computer-assisted sense making —making sense of new data sources — would often be considered secondary deliverables, infrastructure that enables behavior, lowers costs, and improves quality.</para><para>When kfam is applied to primary activities, the questions shift to, <quote>How do I make sense of this situation. What are we trying to accomplish? Who knows what? What doesn't know enough?</quote></para><para> This type of sense making focuses on making sense of a behavioral domain, the performance objectives, and the specific knowledge requirements that enable intelligent behavior.</para><para><computeroutput role="xsol">&lt;kfam/&gt;</computeroutput> element is a customized table that evolved to help the Test subject make sense of:</para><itemizedlist><listitem><para>The ways that knowledge flows through organizations and is acted upon, operationally, planned, and implemented</para></listitem><listitem><para>The various bits and pieces of XML system architectures At its core, kfam is a language for dealing with the fractal nature of language and behavior.</para></listitem><listitem><para>Competing, multidimensional value optimizations. The variety of performance objectives and the specific knowledge requirements that enable intelligent behavior within a span of control.</para></listitem></itemizedlist><para>When engineering markup systems, analysis of knowledge flows has impacts on architectural decisions, fine-grained markup decisions, usability, and occasionally how to deal with organizational dynamics.</para><para>By being closely associated with primary work products and being rather abstract, the meaning of kfam concepts has been highly-dynamic, making formalization an iterative process. Of all of the systems built on the prodoc platform, kfam has had the most iterations and has the most moving parts. It is the most expansive example of semantic authoring being used to create markup based on a person's conceptual models. </para><blockquote><attribution>Test subject</attribution><para>I started working with markup at the same time that Bo introduced me to knowledge management and Joe, values-based decision making. I couldn't untangle those conceptual frameworks with a Lampson crane.</para></blockquote><section><title>kfam conceptual language</title><para>kfam's organized around three core concepts: <phrase role="agent">agents</phrase>, <phrase role="artifact">artifacts</phrase>, and <phrase role="behavior">behaviors</phrase>. Those three <phrase role="artifact">concepts</phrase> are sufficient to describe a <phrase role="behavior">knowledge flow</phrase>.</para><para><phrase role="agent">Agents</phrase><phrase role="behavior">act</phrase>, <phrase role="behavior">make decisions</phrase>, and <phrase role="behavior">exhibit behavior</phrase>. The set of <phrase role="agent">agents</phrase> comprise <phrase role="ind">individuals</phrase>, <orgname>organizations</orgname>, <phrase role="sys">systems</phrase>, and <phrase role="bot">automated agents</phrase> (<phrase role="bot">bots</phrase>).</para><para><phrase role="artifact">Artifacts</phrase> are <phrase role="pa">physical</phrase> or <phrase role="ka">conceptual</phrase> objects. </para><para><phrase role="k">Knowledge</phrase> can also be characterized through such distinctions as</para><itemizedlist><listitem><para>Knowledge artifacts (ka) and meta-knowledge artifacts (mka)</para></listitem><listitem><para><phrase role="data">Data</phrase>, <phrase role="info">information</phrase>, <phrase role="k">knowledge</phrase></para></listitem><listitem><para>Explicit, implicit, and tacit forms</para></listitem></itemizedlist><para><phrase role="behavior">Behavior</phrase> comprises actions and decisions. Knowledge enables behavior. Co-location of knowledge and decisions generally improves performance. Behavior goes by many different names in different contexts.</para><para>Policy is what you do. Policy is behavior. Written policies are only guidance. The pages don't act. Ideally, they are semantic triggers, triggering the intended behaviors by the acting agents.</para><para>Meaning is behavior. Words have no meaning without action. Knowledge flows and lifecycles comprise such behaviors as knowledge creation, retention, transfer, use, and destruction.</para><para>Values are disassociated and abstracted knowledge systems that synthesize pattern recognition and evaluation to quickly apply established logic to operational details with minimal thought. They represent distilled and automated behaviors. Unscripted life requires active management systems; automate the operational details. Novelty focuses active attention, thinking, and — if there isn't too much fear — creativity.</para><para>While <phrase role="agent">agent</phrase>, <phrase role="artifact">artifact</phrase>, and <phrase role="behavior">behavior</phrase> are enough to describe a knowledge flow, additional details help with detailed analysis and engineering. The following diagram relates the kfam <quote>data ∫ information ∫ knowledge ∈ behavior</quote> model with Sowa's concepts and OODA loops, the process that fighter pilots use to Observe, Orient, Decide, and Act.</para><figure><title>Knowledge flows associated with knowledge enabling behavior</title><mediaobject><alt>Observe relates to kfam sensing behaviors. Orient to sense making. Decide and act to knowledge use.</alt><imageobject><imagedata align="center" fileref="img/kfam.ooda.png" width="100%"/></imageobject></mediaobject></figure></section><section><title>kfam markup language</title><para>In markup systems, elements are first-class objects. Early experiments with extending and customizing semantics using attributes proved unworkable due to accessibility issues.</para><para>A couple of other design principles: The egg carton principle. How many eggs would you buy if you had to create your own egg cartons, on the spot? Be generous when you build containers. Redundancy, many ways to say the same thing, doesn't hurt.</para><para>Formalizing kfam starts by creating <computeroutput role="xpe">%spans;</computeroutput> based on the concepts, e.g., <computeroutput role="xsol">&lt;agent/&gt;</computeroutput>, <computeroutput role="xsol">&lt;artifact/&gt;</computeroutput>, <computeroutput role="xsol">&lt;behavior/&gt;</computeroutput>. Likewise for attributes. <computeroutput role="xpe">%semantic-atl;</computeroutput> includes <computeroutput role="xa">@agent</computeroutput>, <computeroutput role="xa">@artifact</computeroutput>, and <computeroutput role="xa">@behavior</computeroutput>.</para><para>Most modeling is done in a specialized table called <computeroutput role="xsol">&lt;kfam/&gt;</computeroutput>, which uses a restricted element set and has a fairly extensive set of controls:</para><figure><title><computeroutput role="xsol">&lt;kfam/&gt;</computeroutput> workspace</title><mediaobject><alt>Four rows of CSS controls adjust table properties. They sit above a kfam table with light formatting</alt><imageobject><imagedata align="center" fileref="img/kfam.workspace.png" width="100%"/></imageobject></mediaobject></figure></section><section><title>kfam visual language</title><para>The kfam visual language starts with colors. Named colors are defined with HSL values and variables in LESS stylesheets. The names are defined for use as attribute values in prodoc.dtd so that they can be applied to <computeroutput role="xa">@color</computeroutput> and <computeroutput role="xa">@bgcolor</computeroutput> using picklists. One of the color pallets is for modeling knowledge flows.</para><figure><title>Named foreground and background colors</title><mediaobject><alt>foreground and background colors for agent, artifact, and behavior</alt><imageobject><imagedata align="center" fileref="img/table.bgcolors.png" width="50%"/></imageobject></mediaobject></figure><para>Another attribute, <computeroutput role="xa">@kstyle</computeroutput>, applies <computeroutput role="xsol">&lt;kfam/&gt;</computeroutput>visualizations to any element. <computeroutput role="xa">@kstyle</computeroutput> accepts the values <computeroutput role="xpath">(bgcolor| color| custom| dark| full| gray| kflow| light| line| none)</computeroutput>. It applies background, foreground, and border color combinations based on the value of the <computeroutput role="xa">@k</computeroutput> attribute, which describes the element's role in knowledge flows. prodoc.dtd sets default <computeroutput role="xa">@k</computeroutput> attribute values for most spans, which can be overwritten to lie to the author, e.g., <computeroutput role="xpath">agent/@k="behavior"</computeroutput>, would create a secret <computeroutput role="xsol">&lt;agent/&gt;</computeroutput> camouflaged to look like a <computeroutput role="xsol">&lt;behavior/&gt;</computeroutput>.</para><figure><title><computeroutput role="xa">@k</computeroutput> → <computeroutput role="xa">@kstyle</computeroutput> CSS style mappings</title><mediaobject><alt>Setting the @k attribute to "agent" maps any element to the class of agents</alt><imageobject><imagedata align="center" fileref="img/kfam.k-kstyle.png" width="70%"/></imageobject></mediaobject></figure><figure><title><computeroutput role="xa">@kstyle</computeroutput> variations applied to a table</title><mediaobject><alt>The @kstyle attribute can be changed to get a range of visual effects</alt><imageobject><imagedata align="center" fileref="img/table.kstyleVariations.png" scale="50"/></imageobject></mediaobject></figure></section><section><title>kfam modeling</title><para>kfam started as a natural language for discussing project activities and how knowledge enables behavior. Some of that ambiguity has remained as the language has evolved into an analytical system, modeling language, and engineering framework. The most complete, published definition of kfam can be found in <link xl:href="https://sagebrushgroup.com/archive/201009_KFlowAnalysisModel_Paper.pdf"><citetitle>Knowledge Flow Modeling and Analysis with Focus on Enabling Actions and Decisions within the Business Process</citetitle></link>.</para><para>kfam was first used as a modeling language when markup program managers were being blamed for expected IT schedule slippages. Finding the solution was pretty easy.</para><orderedlist><listitem><para>Map out the processing steps, defining each of the transforms and intermediate data forms</para></listitem><listitem><para>Identify the responsible agents. Who was doing the work? A person? A bot? Some combination?</para></listitem><listitem><para>Any knowledge gaps or other barriers to progress?</para></listitem><listitem><para>Where does any missing information need to come from?</para></listitem></orderedlist><figure><title>Knowledge gap analysis</title><mediaobject><alt>Source documents are converted to targets through a series of transformations and intermediate forms. The acting agents have knowledge gaps, and need the assistance of knowledgeable sources</alt><imageobject><imagedata align="center" fileref="img/kfam.gapAnalysis.png" width="100%"/></imageobject></mediaobject></figure><para>It turned out that all of the knowledge gaps were pending IT decisions. The problem went away. We also made a couple of refinements to the startup process.</para><para>Here's a more-complete set of kfam elements: Any element can join a class by setting the <computeroutput role="xa">@k</computeroutput> attribute to the listed value.</para><figure><title><computeroutput role="xa">@k</computeroutput> values and associated colors</title><mediaobject><alt>Agent colors are in the red-orange range. Artifacts, blue-purple. Behaviors use greens and yellows</alt><imageobject><imagedata align="center" fileref="img/kfam.kfamColors.png" scale="50"/></imageobject></mediaobject></figure></section></section><section><title><computeroutput role="xsol">&lt;music/&gt;</computeroutput> — Rationalizing chord/ lyric pairings</title><para>When chord symbols lose alignment with the lyrics, they lose their meaning. With WP, the pairings regularly fall out of alignment for countless reasons:</para><figure><title>Lyric-chord charts with misaligned text (it gets much worse, especially with proportional fonts)</title><mediaobject><alt>Shows the F#m chord above the word "Me" preventing the next chord "G" from aligning with the word "eat"</alt><imageobject><imagedata align="center" fileref="img/codeblock.oldMusic.png" width="50%"/></imageobject></mediaobject></figure><para>Markup was used to make the chord <computeroutput role="xsol">&lt;c/&gt;</computeroutput> and lyric <computeroutput role="xsol">&lt;l/&gt;</computeroutput> relationships more explicit.</para><figure><title><computeroutput role="xsol">&lt;lyric/&gt;</computeroutput> lines, with lyric fragments <computeroutput role="xsol">&lt;l/&gt;</computeroutput>, and chord <computeroutput role="xsol">&lt;c/&gt;</computeroutput>symbols next to each other so they don't get lost</title><literallayout><computeroutput>&lt;music&gt;
  &lt;lyric&gt;
    &lt;l&gt;In &lt;/l&gt;
    &lt;c&gt;D&lt;/c&gt;
    &lt;l&gt;remembrance of &lt;/l&gt;
    &lt;c&gt;F#m&lt;/c&gt;
    &lt;l&gt;Me &lt;/l&gt;
    &lt;c&gt;G&lt;/c&gt;
    &lt;l&gt;eat this &lt;/l&gt;
    &lt;c&gt;A&lt;/c&gt;
    &lt;l&gt;bread&lt;/l&gt;
  &lt;/lyric&gt;
&lt;/music&gt;</computeroutput></literallayout></figure><para>XPath was used to manage horizontal and vertical offsets.</para><figure><title>CSS rendering with fully-automated offsets</title><mediaobject><alt>Shows the G chord correctly aligned with the word "eat"</alt><imageobject><imagedata align="center" fileref="img/music.cssRendering.png" width="60%"/></imageobject></mediaobject></figure><para>All the CSS gymnastics cause cursor-positioning problems. Adding an edit mode solved the problem:</para><figure><title><computeroutput role="xpath">music[@view="edit"]</computeroutput> markup</title><programlisting>&lt;music view="edit"&gt;
  &lt;lyric&gt;
    &lt;l&gt;In &lt;/l&gt;&lt;c&gt;D&lt;/c&gt;&lt;l&gt;remembrance of &lt;/l&gt;&lt;c&gt;F#m&lt;/c&gt;&lt;l&gt;Me&amp;nbsp;&amp;nbsp;&lt;/l&gt;
    &lt;c&gt;G&lt;/c&gt;&lt;l&gt;eat this &lt;/l&gt;&lt;c&gt;A&lt;/c&gt;&lt;l&gt;bread&lt;/l&gt;
  &lt;/lyric&gt;
&lt;/music&gt;</programlisting></figure><figure><title><computeroutput role="xpath">music[@view="edit"]</computeroutput> rendering</title><mediaobject><alt>Shows the chords and words on one line to simplify editing</alt><imageobject><imagedata align="center" fileref="img/music.cssEdit.png" width="70%"/></imageobject></mediaobject></figure></section><section><title><computeroutput role="xsol">&lt;vcanvas/&gt;</computeroutput>
        — Visualizing and comparing sets of value optimizations</title><para>Value canvases come from the book <citetitle>Blue Ocean Strategy</citetitle>, They provide a good way to visualize and compare multiple sets of value propositions. The following SVG rendering was generated from data entered into a <computeroutput role="xsol">&lt;vcanvas/&gt;</computeroutput> table.</para><figure><title>Rendering of <computeroutput role="xsol">&lt;vcanvas/&gt;</computeroutput> datasets</title><para><inlinemediaobject><alt>Value canvas comparing text, word, and XML value propositions</alt><imageobject><imagedata fileref="img/vcanvas.xmlValue.png" scale="30"/></imageobject></inlinemediaobject></para></figure></section><section><title>WordNet and SUMO integrations — Associating markup with dictionaries &amp; formal logic</title><para>There are different ways to organize bottom-up markup. A technical solution is namespaces, but that just keeps the chaos from bugging the bots.</para><blockquote><attribution>Anonymous source</attribution><para>Namespaces! A linguistic solution without behavioral context (e.g., requirements). So many different implementation models. So much fun.</para></blockquote><para>Another approach is negotiations. As individual markup is shared and encounters similar/ overlapping markup, conversations are used to generalize the concepts and markup. Different concepts get different names. Individual markup feeds departmental standards. Semantic generalization flows up through layered DTDs, matching the organizational hierarchy. This represents an evolutionary approach to bottom-up data modeling and system design.</para><para>Coming at the question from the opposite direction, individual markup could be anchored to a separate, semantic authority. At the first Ontolog face-to-face meeting &amp; workshop in 2003, Adam Pease introduced SUMO, the Suggested Upper Merged Ontology to the group. The aftermath is described in the forward to In Adam's book <citetitle>Ontology</citetitle>:</para><blockquote><attribution>Duane Nickull</attribution><para>The result of [Adam's] behavior was quite infectious. Adam, Kurt Conrad, and I ended up in a late night sushi restaurant somewhere near Menlo Park, CA, discussing how to map SUMO concepts to Mandarin, Japanese, and Cantonese, how WordNet can reference SUMO, and why First-Order Logic (FOL) constraints are generally a cool concept to have in advanced computer systems. An upper-level ontology, such as SUMO, is a common, shared conceptualization of a domain.</para></blockquote><section><title>Approach</title><para>The draft implementation uses prodoc's document-level extension mechanism to add global attributes a test document:</para><figure><title>Semantic formalization attribute list</title><literallayout><computeroutput>&lt;!-- ren to fsem.atl when generalized --&gt;
&lt;!ENTITY  % sa.atl
"
              shsem           (show| hidden)   #IMPLIED
              sumoID          CDATA            #IMPLIED
              sumoLogic       CDATA            #IMPLIED
              sumoText        CDATA            #IMPLIED
              userID          CDATA            #IMPLIED
              userLogic       CDATA            #IMPLIED
              userText        CDATA            #IMPLIED
              wnetID          CDATA            #IMPLIED
              wnetSense       CDATA            #IMPLIED
" &gt;</computeroutput></literallayout></figure><para>The approach was tested with the concepts <computeroutput role="xsol">&lt;agent/&gt;</computeroutput>, <computeroutput role="xsol">&lt;artifact/&gt;</computeroutput>, and <computeroutput role="xsol">&lt;behavior/&gt;</computeroutput>. The <computeroutput role="xa">@shsem</computeroutput> attribute controls the expansion and collapsing of the associated form control that provides access to the WordNet and SUMO attributes set.</para><figure><title>Elements mapped to WordNet definitions and SUMO logic</title><mediaobject><alt>A behavior element has been expand to show interfaces for WordNet and SUMO connections</alt><imageobject><imagedata align="center" fileref="img/tl.elements2sumo.png" width="70%"/></imageobject></mediaobject></figure></section><section><title>Findings &amp; next steps</title><blockquote><attribution>Roger C. Schank, cognitive scientist</attribution><para>Humans are not ideally set up to understand logic; they are ideally set up to understand stories.</para></blockquote><para>An obvious next step is to connect the form control to a SUMO repository and add a lookup/ query interface. The level of effort appears reasonable.</para><para>More thought needs to go into a more challenging issue, authoring fully-formalized semantics. Spelling KIF is one thing. Spelling out formal logic in KIF is another.</para><para>Semantic markup can simplify knowledge transfer by making implicit organizing concepts more explicit. This is especially useful during learning, before a new conceptual framework's language and logic have been fully-internalized.</para><para>Expressing KIF concepts more explicitly through markup is expected to help it make sense in more behavioral contexts:</para><itemizedlist><listitem><para>For individuals, faster comprehension and productivity, especially when style variations can be applied to help differentiate concepts</para></listitem><listitem><para>For the bots, more tool options. XSLT and XPath, by themselves, dramatically expand the software development options.</para></listitem></itemizedlist><para>In his <citetitle>Standard Upper Ontology Knowledge Interchange Format</citetitle> document, Adam describes SUO-KIF using BNF syntax:</para><figure><title>SUO-KIF definition</title><literallayout><computeroutput>upper ::= A | B | C | D | E | F | G | H | I | J | K | L | M |
N | O | P | Q | R | S | T | U | V | W | X | Y | Z
lower ::= a | b | c | d | e | f | g | h | i | j | k | l | m |
n | o | p | q | r | s | t | u | v | w | x | y | z
digit ::= 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
special ::= ! | $ | % | &amp; | * | + | - | . | / | &lt; | = | &gt; | ? |
@ | _ | ~ |
white ::= space | tab | return | linefeed | page
initialchar ::= upper | lower
wordchar ::= upper | lower | digit | - | _ | special
character ::= upper | lower | digit | special | white
word ::= initialchar wordchar*
string ::= "character*"
variable ::= ?word | @word
number ::= [-] digit+ [. digit+] [exponent]
exponent ::= e [-] digit+
term ::= variable | word | string | funterm | number | sentence
relword ::= initialchar wordchar*
funword ::= initialchar wordchar*
funterm ::= (funword term+) | (funword sentence+)
sentence ::= word | equation | inequality |
relsent | logsent | quantsent
equation ::= (= term term)
relsent ::= (relword term+) | (relword sentence+)
logsent ::= (not sentence) |
(and sentence+) |
(or sentence+) |
(=&gt; sentence sentence) |
(&lt;=&gt; sentence sentence)
quantsent ::= (forall (variable+) sentence) |
(exists (variable+) sentence)</computeroutput></literallayout></figure><para>ixml was used to transform KIF to <computeroutput role="xsol">&lt;kif/&gt;</computeroutput>. The most significant finding was that the definition appears to include a couple of unnamed layers that deserve clarification.</para><para>Integrations with other automated systems have been explored. OntoInsights focuses on the value propositions associated with storytelling and deep narrative analysis. It generates RDF fragments that can be translated into KIF.</para><figure><title>From semi-formalized markup languages to fully-formalized ontologies</title><mediaobject><alt>A developer creates the kif markup language. An author creates markup and definitions to feed a bot that generates kif markup, which feeds editorial and negotiation processes</alt><imageobject><imagedata align="center" fileref="img/kfam.formalization.png" width="100%"/></imageobject></mediaobject></figure><blockquote><attribution>Test subject</attribution><para>Every time I look in one of those fully-formalized thingies, it never agrees with the way I think.</para></blockquote></section></section></section><section><title>Bottom-up negotiations to define shared meanings</title><blockquote><attribution>Paul Strassmann, The Politics of Information Management</attribution><para>Policy is what you do, not what you say or write.</para></blockquote><para>A few more mantras from Strassmann: <quote>Information management is primarily a question of politics and secondarily one of technology.</quote><quote>Operational decisions take care of today. Management decisions protect the future.</quote><quote>Without privacy, there is no freedom.</quote></para><para>Negotiations around markup are rarely zero-sum. The ability to handle optional structures usually provides sufficient flexibility to accommodate different stakeholder interests. Even so, as the complexity of the project increases, integrating the multiple perspectives becomes more challenging.</para><para>Just as bottom-up markup starts with defining semantic values, a parallel, bottom-up approach to decision making starts with conversations to define common human values.</para><para>As the discussion topics progress, the group creates a shared vision for change and looks at the current conflicts from a position of safety. Issues are prioritized, and initiatives defined — quickly and without conflict.</para><para>More importantly, the process is energizing. Individuals see their values in the shared vision, and the new initiatives as a way to accomplish personal objectives. </para><para>From an ontological perspective, the identification, articulation, and prioritization of abstract values, starts a formalization process. The corresponding conversations create shared meanings to support the new language.</para><para>The following training aide describes the process in more detail:</para><informaltable orient="land"><tbody><tr><td><figure><title>Community engagement process</title><mediaobject><alt>Four columns provide guidance on assembling a community, conversing and forming consensus, collaborating, and creating change</alt><imageobject><imagedata fileref="img/ceProcess.png" width="100%" contentdepth="100%" scalefit="1"/></imageobject></mediaobject></figure></td></tr></tbody></informaltable></section><section><title>h1.dtd — Build your own prodoc</title><para><filename>h1.dtd</filename> is based on an HTML subset. It's scheduled for release on June 10th.</para><section><title>h1.dtd summary</title><programlisting>&lt;!-- ==========================================================================
                                                              dawkIns ....
              dawk Summary            
                                               
    SPath:    D:\kc\xt\h1\                  
    SFile:    h1.dtd                
    Gen:      2023-05-30 22:12         
    GenBy:    D:\kc\xt\bin\dawk.awk           
    Target:   D:\kc\xt\bin\scratch\dawk_tgt.txt           
    fileType: xml
    chopCol:  79

    Module:   h1.dtd
              HTML 1 Document Type Declarations

    Public Identifier:

              -//SG//DTD h1//EN

    Date:     2021.05.29

    Version:  Released for testing
 
    Contents:

   175    Declaration constants (parameter entity declarations)
   178   
   179      Extensible element constants
   182   
   183        %divs; - infinitely-nesting wrapper elements
   186                %h1.divs;           baseline hierarchical divisions
   194                %sa.divs;           semantic authoring interface
   198                %divs;              (roll-up)
   203   
   204        %heads; - top of div (metadata) elements
   207                %h1.heads;          baseline heading containers
   211                %sa.heads;          semantic authoring interface
   215                %heads;             (roll-up)
   220   
   221        %blocks; - paragraph-level elements
   224                %h1.blocks;         baseline blocks (email benchmark)
   239                %sa.blocks;         semantic authoring interface
   243                %blocks;            (roll-up)
   248   
   249        %spans; - inline elements
   252                %h1.spans;          baseline spans
   280                %sa.spans;          semantic authoring interface
   284                %spans;             (roll-up)
   289   
   290      Attribute constants
   293   
   294        Semantic attribute entities
   297                @alt                alternate (text for image)
   301                @agent              acts or has the power to act
   305                @artifact           physical or conceptual object
   309                @author             agent responsible for writing text
   313                @behavior           an agent's actions and decisions
   317                @class              css classifications
   321                @content            add @name for custom "attribute"
   325                @created            when created
   329                @href               hypetext reference
   333                @id                 unique identifier
   337                @idref              reference to a single identifier
   341                @modified           when modified
   345                @name               add @content for matching value
   349                @src                media source
   353                @status             relative ranking
   357                @tags               #words that classify or categorize
   361                @type               nature or genre
   365                %sem.atl;           (roll-up)
   385   
   386        Enumerated attribute value sets for style attributes
   389                %avs.align;         horizontal text alignment values
   394                %avs.display        display property values
   402                %avs.open.closed;   details open/closed switch values
   406                %avs.valign;        vertical alignment values
   412                %avs.vwhite         vertical whitespace
   417                %avs.whitespace     text whitespace
   422   
   423        Style attribute entities
   426                @align              horizontal text alignment
   430                @bgcolor            background color
   434                @border             (width style color)
   438                @color              text color
   442                @colspan            column span
   446                @display            default display geometry is inline
   450                %a.display.def;     defined default (e.g., "block")
   454                @height             vertical size
   458                @margin             whitespace ouside border
   462                @open               open/close display &amp; displayblock
   466                @padding            whitespace inside border
   470                @rowspan            row span
   474                @scale              scales font size
   478                @style              inline css
   482                @valign             vertical alignment
   486                @white              text whitespace
   490                @width              horizontal size
   494                @xspace.pre         xml:space="preserve"
   499                %style.atl;         (roll-up)
   517   
   518        %gatl; - extensible global attribute set
   531                %sa.atl;            semantic authoring interface
   535                %gatl;               (roll-up)
   541   
   542      Element content model constants
   545                %div.model;         generalized content model for divs
   549                %fig.model;         content model for figure element
   553                %kitchen.sink;      merged content models
   557                %mixed;             parsable character data and %spans;
   561                %text;              parsable character data content model
   565                %undefined;         text
   569   
   570    Element and attlist declarations
   573   
   574      %h1.divs; declarations
   577                div                 nested, hierarchical division
   583                h1                  H1 wrapper
   589                hsa                 HTML for semantic authoring wrapper
   595                hsg                 HTML for semantic generalization wrap
   601                prodoc              prodoc wrapper
   607   
   608      %h1.heads; declarations
   611                h                   heading
   617   
   618      %h1.blocks; declarations
   621                block               generic block of text
   627                codeblock           block of source code
   634                figure              wrapper bordered content
   639                hr                  horizontal rule, thematic break
   644                ol                  ordered list wrapper
   649                p                   paragraph
   654                qblock              block formatting for quoted text
   659                t                   text
   665                table               tablular, matrix data
   670                  tr                table row
   675                tl                  text list
   680                ul                  unordered list
   685   
   686      %h1.spans; declarations
   689                a                   hyperlink anchor
   694                agent               one who acts, that which acts
   699                artifact            physical or conceptual object
   704                b                   bold text
   709                behavior            action, decision, patterns
   714                big                 big text
   719                br                  line break
   724                details             @open="closed" hides all but summary
   729                i                   italic text
   734                img                 inline image
   742                meta                meta-data
   747                name                designates and references concept
   752                nowrap              non-breaking text
   757                q                   quotation
   762                s                   strikeout
   767                small               small text
   772                source              origin, reference
   777                span                generic inline, span of text
   782                strong              important text
   787                summary             visible heading for details element
   792                target              goal, result, focus
   797                td                  table data cell
   802                th                  table header cell
   807                u                   underline
   812   
   813    Document constants (general entity declarations)
   816                &amp;bull;            • bullet
   820                &amp;circle;          ∘ ring operator
   824                &amp;emdash;          — emdash symbol
   828                &amp;h1;                "h1" string
   832                &amp;H1;                "H1" string
   836                &amp;hsa;               "hsa" string
   840                &amp;Hsa;               "Hsa" string
   844                &amp;nbsp;              non-breaking space
  1034   
  1035    END  h1.dtd

# 2023.05.30 22:12 # D:\kc\xt\h1\ # h1.dtd #

                                                              dawkIns .....
=========================================================================== --&gt;</programlisting></section></section></article>